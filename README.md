# papers_of_interest
Papers I'd ideally read, given enough time.

## Deep learning

## Spiking neural networks

Title: __Towards Scalable, Efficient and Accurate Deep Spiking Neural Networks with Backward Residual Connections, Stochastic Softmax and Hybridization__<br/>
Authors: __Priyadarshini Panda, Aparna Aketi, and Kaushik Roy__<br/>
https://arxiv.org/abs/1910.13931
<details>
<summary>Abstract</summary>
Spiking Neural Networks (SNNs) may offer an energy-efficient alternative for
implementing deep learning applications. In recent years, there have been
several proposals focused on supervised (conversion, spike-based gradient
descent) and unsupervised (spike timing dependent plasticity) training methods
to improve the accuracy of SNNs on large-scale tasks. However, each of these
methods suffer from scalability, latency and accuracy limitations. In this
paper, we propose novel algorithmic techniques of modifying the SNN
configuration with backward residual connections, stochastic softmax and hybrid
artificial-and-spiking neuronal activations to improve the learning ability of
the training methodologies to yield competitive accuracy, while, yielding large
efficiency gains over their artificial counterparts. Note, artificial
counterparts refer to conventional deep learning/artificial neural networks.
Our techniques apply to VGG/Residual architectures, and are compatible with all
forms of training methodologies. Our analysis reveals that the proposed
solutions yield near state-of-the-art accuracy with significant
energy-efficiency and reduced parameter overhead translating to hardware
improvements on complex visual recognition tasks, such as, CIFAR10, Imagenet
datatsets.
</details>




## Neuroscience-inspired neural networks

Title: __Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks__<br/>
Authors: __Roman Pogodin, Dane Corneil, Alexander Seeholzer, Joseph Heng, Wulfram Gerstner__<br/>
https://arxiv.org/abs/1910.10559
<details>
<summary>Abstract</summary>
Reservoir computing is a powerful tool to explain how the brain learns
temporal sequences, such as movements, but existing learning schemes are either
biologically implausible or too inefficient to explain animal performance. We
show that a network can learn complicated sequences with a reward-modulated
Hebbian learning rule if the network of reservoir neurons is combined with a
second network that serves as a dynamic working memory and provides a
spatio-temporal backbone signal to the reservoir. In combination with the
working memory, reward-modulated Hebbian learning of the readout neurons
performs as well as FORCE learning, but with the advantage of a biologically
plausible interpretation of both the learning rule and the learning paradigm.
</details>

Title: __CTNN: Corticothalamic-inspired neural network__<br/>
Authors: __Leendert A Remmelzwaal, Amit Mishra, George F R Ellis__<br/>
https://arxiv.org/abs/1910.12492
<details>
<summary>Abstract</summary>
Sensory predictions by the brain in all modalities take place as a result of
bottom-up and top-down connections both in the neocortex and between the
neocortex and the thalamus. The bottom-up connections in the cortex are
responsible for learning, pattern recognition, and object classification, and
have been widely modelled using artificial neural networks (ANNs). Current
neural network models (such as predictive coding models) have poor processing
efficiency, and are limited to one input type, neither of which is
bio-realistic. Here, we present a neural network architecture modelled on the
corticothalamic connections and the behaviour of the thalamus: a
corticothalamic neural network (CTNN). The CTNN presented in this paper
consists of an auto-encoder connected to a difference engine, which is inspired
by the behaviour of the thalamus. We demonstrate that the CTNN is input
agnostic, multi-modal, robust during partial occlusion of one or more sensory
inputs, and has significantly higher processing efficiency than other
predictive coding models, proportional to the number of sequentially similar
inputs in a sequence. This research helps us understand how the human brain is
able to provide contextual awareness to an object in the field of perception,
handle robustness in a case of partial sensory occlusion, and achieve a high
degree of autonomous behaviour while completing complex tasks such as driving a
car.
</details>

## Artificial life and open-ended evolution

## Neuroscience

